# 평가 기준 템플릿 허브: 최종 통합 제안서

## 1. 서론

본 문서는 CR 프로젝트의 '평가 기준 템플릿 허브 사이트' 구축을 위해 제안된 여러 아이디어를 종합적으로 분석하고, 각 제안의 강점을 극대화한 최종 통합안을 제시합니다. 이 제안서는 **빠른 실행 가능성**, **평가 품질 확보**, **장기적 지속가능성**이라는 세 가지 핵심 목표의 균형을 맞추는 데 중점을 둡니다.

---

## 2. 핵심 설계 철학: '교육적 유도'와 '선택권 존중'의 조화

프로젝트의 성공은 양질의 평가 데이터 확보에 달려있습니다. 이를 위해 본 통합안은 **'평가 품질 우선'** 원칙을 채택하되, 사용자의 반감을 사지 않는 정교한 접근 방식을 제안합니다.

> **"사용자를 강제하는 것이 아니라, 명확한 데이터와 교육적 가이드를 통해 최적의 선택을 하도록 돕는다."

- **투명한 정보 제공**: 왜 특정 AI가 더 나은지 객관적인 데이터(성능 점수, 기능 차이)를 투명하게 공개합니다. 

- **강력한 유도 (Nudging)**: 시각적 강조(버튼 색상, 크기)와 기본값 설정을 통해 사용자가 자연스럽게 최적의 경로를 선택하도록 유도합니다. 

- **궁극적 선택권 보장**: 사용자가 경고를 인지한 상태에서 비최적 환경을 선택할 자유를 보장하되, 그에 따른 한계점을 명확히 안내합니다.

- **교육적 접근**: 사용자가 AI 리터러시를 높이고, 프로젝트의 파트너로서 성장할 수 있도록 돕는 교육적 콘텐츠를 제공합니다. 

---

## 3. 최종 제안 플로우 (The Integrated Flow)

각 제안의 장점을 결합한 최종 사용자 플로우는 다음과 같습니다.

```
[시작]
 ↓
1. 기사 URL 입력 및 AI 환경 파악
   - "분석할 기사 주소를 입력해주세요."
   - "현재 주로 사용하는 AI 서비스는 무엇인가요?" (선택: Claude, ChatGPT, Gemini, 기타, 없음) (친구 D)
 ↓
2. 기사 자동 분석 및 유형 확인
   - (로딩) "기사를 분석하고 있습니다..."
   - "AI가 이 기사를 [스트레이트 뉴스]로 분석했습니다. 정확한가요?" (친구 C)
   - [네, 맞습니다] [아니요, 직접 수정할게요 → 유형 선택] (친구 C)
 ↓
3. 최적 환경 추천 및 성능 비교
   - (분석 결과 + 추천 AI 강조 표시) (친구 B, D)
   - ┌───────────────────────────────────┐
   - │ 📰 분석 결과: 스트레이트 뉴스, 사실 확인 중심 │
   - │ ✅ 권장 AI: Claude 3.5 Sonnet (예상 품질: ★★★★★) │
   - │ [💎 Claude로 평가하기] ← 주 버튼 (친구 B) │
   - │                                           │
   - │ 현재 사용 AI: ChatGPT-4o (예상 품질: ★★★★☆) │
   - │ [내 AI로 계속하기] [상세 비교 보기] (친구 D) │
   - └───────────────────────────────────┘
 ↓
4. 차별화된 템플릿 및 가이드 제공 (친구 D)
   - (A) 'Claude로 평가하기' 선택 시:
     - ✅ 최고 품질 템플릿 세트 (최적화된 프롬프트, 상세 가이드, 예시 리포트)
   - (B) '내 AI로 계속하기' 선택 시:
     - ⚠️ "선택하신 환경은 최적 성능이 아닐 수 있습니다."
     - ✅ 기본 템플릿 + 해당 AI용 가이드 + **한계점 및 보완 방법 안내 문서**
 ↓
5. 다운로드 완료 및 피드백 요청
   - "다운로드가 완료되었습니다."
   - "평가 완료 후, 결과를 이곳에 제출하여 프로젝트에 기여해주세요!" (구글 폼 링크) (친구 C)
   - "다른 평가자들의 결과가 궁금하다면? [커뮤니티 방문하기]" (친구 D)
[종료]
```

---

## 4. 단계별 개발 로드맵 (Phased Approach)

모든 기능을 한 번에 구현하는 것은 비효율적입니다. 따라서, **'빠른 출시 → 점진적 고도화'** 전략을 채택하여 3단계로 나누어 개발할 것을 제안합니다.

### **Phase 1: MVP - 핵심 기능 구현 (4~6주)**

**목표**: 가장 기본적인 기능만으로 빠르게 출시하여 사용자 피드백을 수집하고 시장성을 검증합니다.

| 기능 | 설명 | 비고 |
| --- | --- | --- |
| **기사 URL 입력 및 스크래핑** | 사용자가 URL을 입력하면 기사 본문을 추출합니다. |  |
| **기사 유형 자동 분류 (LLM API)** | 저비용 LLM API(Claude Haiku 등)를 활용해 기사 유형을 분류합니다. |  |
| **기사 유형 수동 수정** | AI의 분류가 틀렸을 경우 사용자가 직접 수정할 수 있는 기능을 제공합니다. | **** |
| **AI 서비스 선택** | 사용자가 자신이 사용할 AI를 드롭다운 메뉴에서 직접 선택합니다. |  |
| **기본 템플릿 제공** | 선택된 (기사 유형 x AI 서비스) 조합에 맞는 기본 템플릿과 가이드를 제공합니다. |  |
| **피드백 수집 (외부 폼)** | 구글 폼 또는 Airtable을 연동하여 사용자의 평가 결과를 수집합니다. | **** |

**기술 스택 (Phase 1)**:

- **프론트엔드**: Next.js 또는 SvelteKit

- **백엔드**: Python (FastAPI) + BeautifulSoup/Requests

- **기사 분류**: 외부 LLM API (Anthropic/OpenAI)

- **데이터 관리**: JSON 파일 (템플릿 매핑), Google Forms (피드백)

### **Phase 2: 품질 강화 (추가 6~8주)**

**목표**: MVP에서 수집된 데이터를 바탕으로, 평가 품질을 높이기 위한 유도 장치를 도입합니다.

| 기능 | 설명 | 비고 |
| --- | --- | --- |
| **AI 성능 데이터베이스 구축** | 각 AI 모델의 기사 유형별 예상 성능 점수를 관리하는 DB를 구축합니다. | **** |
| **최적 환경 추천 시스템** | 기사 분석 결과를 바탕으로 최적의 AI를 추천하고, 시각적으로 강조(주 버튼)합니다. |  |
| **성능 비교 데이터 제시** | 사용자가 다른 AI를 선택하려 할 때, 예상 성능 점수와 한계점을 명확히 보여줍니다. |  |
| **사용자 선택 데이터 수집** | 사용자가 어떤 경로(추천/비추천)를 선택하는지 데이터를 수집하여 분석합니다. |  |

**기술 스택 (Phase 2 추가)**:

- **데이터베이스**: PostgreSQL 또는 Supabase (AI 성능 데이터, 사용자 선택 로그)

- **프론트엔드**: Chart.js (성능 비교 시각화)

### **Phase 3: 생태계 고도화 (추가 8~12주)**

**목표**: 사용자를 단순 이용자에서 프로젝트의 적극적인 참여자로 전환하고, 장기적인 데이터 선순환 구조를 완성합니다.

| 기능 | 설명 | 비고 |
| --- | --- | --- |
| **사전 환경 감지** | 1단계에서 사용자의 AI 환경을 먼저 파악하여 더욱 개인화된 추천을 제공합니다. | ** ** |
| **차별화된 템플릿 패키지** | 최적 환경 사용자에게는 비디오 가이드, 체크리스트 등 'VIP 패키지'를, 비최적 사용자에게는 '보완 가이드'를 제공합니다. | **** |
| **교육적 콘텐츠 허브** | AI 서비스별 장단점, 비용 비교, CR 프로젝트의 중요성 등을 설명하는 별도 페이지를 구축합니다. |  |
| **A/B 테스트 프레임워크** | 어떤 유도 문구나 UI가 더 효과적인지 지속적으로 실험하고 개선합니다. |  |

**기술 스택 (Phase 3 추가)**:

- **프론트엔드**: Framer Motion (애니메이션), Chakra UI (고급 컴포넌트)

- **백엔드**: 템플릿 동적 생성 엔진 (Jinja2 등)

- **분석**: Mixpanel 또는 자체 분석 시스템 (A/B 테스트, 사용자 행동 분석)

---

## 5. 결론 및 기대 효과

본 통합 제안서는 **'실용적 시작'**과 **'야심찬 비전'**을 결합한 로드맵입니다. 현실적인 MVP 제안으로 빠르게 시작하되, 강력한 품질 관리 및 사용자 참여 전략을 단계적으로 도입함으로써, CR 프로젝트의 핵심 목표인 **'신뢰할 수 있는 시민 주도 언론 비평 생태계 구축'**을 가장 효과적으로 달성할 수 있을 것입니다.

**기대 효과**:

- **단기**: 빠른 서비스 출시로 초기 사용자 확보 및 데이터 축적 시작

- **중기**: 평가 품질의 상향 평준화 및 플랫폼 신뢰도 제고

- **장기**: 사용자의 자발적 참여와 데이터 기여를 통한 지속가능한 성장 및 언론 개혁 동력 확보

